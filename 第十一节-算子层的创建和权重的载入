第十一节-算子层的创建和权重的载入

1. 算子层的创建和权重的载入的权重读取
        在上一节课程中，我们采用了mmap内存映射技术来打开庞大的权重模型文件。在这个文件中，我们的首要任务是读取llama模型的配置文件，该文件包含了如dim、head_dim、vocab size等在模型构建和推理过程中必需的参数。
        紧随配置文件之后的是模型的权重参数，这些主要包括全连接层、多头自注意力层以及rmsnorm归一化层所需的参数。在本节课程中，我们将基于已读取的参数和权重，着手构建各个算子层，我们继续上节课的内容，假设现在已经打开了模型权重文件，先是从中读取配置参数相关的信息：
int32_t fd = open(model_path_.data(), O_RDONLY);
if (fd == -1) {
    return error::PathNotValid("Failed to open the weight file " +     model_path_ + " may be the path does not exist!");
}

FILE* file = fopen(model_path_.data(), "rb");
if (!file) {
    return error::PathNotValid("Failed to open the file. The path may be invalid.");
}

auto config = ModelConfig{};
if (fread(&config, sizeof(ModelConfig), 1, file) != 1) {
    return error::ModelParseError(
        "Failed to retrieve the configuration information from the model "
        "file.");
}
这段代码我们在上节课已经讨论过，现在让我们简要回顾一下。首先，我们使用fopen函数来打开模型文件，然后读取了文件开头的ModelConfig * sizeof(float)大小的内容，这部分内容被用作模型的配置参数，原因是模型权重文件的数据分布就像下面那样。
---------
dim,hidden_dim,layer_num，...  前面的28个字节
---------
float 权重
---------
另外，如果是量化模型，在前面配置参数的位置，我们还需要设置group size，这代表了分组量化的组大小。因此在读取过程中，需要额外再读取4个字节以获取group size的值。
---------
dim,hidden_dim,layer_num，...  前面的28个字节
group size
---------
float 权重
---------
接下来，我们将使用已经打开的文件描述符来映射内存空间。
raw_model_data_->data =
    mmap(nullptr, raw_model_data_->file_size, PROT_READ, MAP_PRIVATE, raw_model_data_->fd, 0);

raw_model_data_->weight_data =
    static_cast<int8_t*>(raw_model_data_->data) + sizeof(ModelConfig) + sizeof(group_size_);
weight_data的位置指向模型权重文件中权重的起始位置。接下来，我们将利用这里的权重数据来初始化所有的算子。
2. 需要初始化的所有算子
我们将用weight_data来初始化以下的这些层，以下的这些层都是我们在之后课程的llama推理中将会用到的，包括全连接层wq_layer,wk_layer和wv_layer它们分别用于求取query矩阵，key矩阵以及value矩阵。
struct LLama2Layers {
  std::shared_ptr<op::Layer> add_layer_;
  std::shared_ptr<op::Layer> rope_layer_;
  std::shared_ptr<op::Layer> swiglu_layer_;
  std::shared_ptr<op::Layer> mha_layer_;

  std::vector<std::shared_ptr<op::Layer>> wq_layers_;
  std::vector<std::shared_ptr<op::Layer>> wk_layers_;
  std::vector<std::shared_ptr<op::Layer>> wv_layers_;
  std::vector<std::shared_ptr<op::Layer>> wo_layers_;

  std::vector<std::shared_ptr<op::Layer>> w1_layers_;
  std::vector<std::shared_ptr<op::Layer>> w2_layers_;
  std::vector<std::shared_ptr<op::Layer>> rmsnorm_layers_;
  std::vector<std::shared_ptr<op::Layer>> w3_layers_;
  std::shared_ptr<op::Layer> cls_layer_;

  std::shared_ptr<op::Layer> embedding_layer_;

  void to_cuda(std::shared_ptr<kernel::CudaConfig> config);
};
另外还有用在mlp层的w1_layer，w2_layer以及w3_layer，另外就是embedding_layer用于将token映射为一组向量，rmsnorm_layer用于对输入做均方根归一化，以上的这些层它们的共同点是都有一组权重值，所以需要我们从weight_data中读取并赋值给这些层的权重。
3. 创建带权重的层
在权重weight_data中有一定的排布顺序，这和export.py中规定的是一样的，首先是加载嵌入式词表层，也就是embedding_layer的权重，在create_param_layers中：
1. 首先是创建embedding layer层；
// 创建层本身
llama_layers_->embedding_layer_ = std::make_shared<op::EmbeddingLayer>(
    device_type_, config_->dim_, \
    config_->seq_len_, std::abs(config_->vocab_size_));
2. 由于该层的权重位于raw_model_data（用mmap打开的权重）的起始位置，所以我们直接通过set_weight的方法将它赋值给新创建的embedding_layer；
  const void* weight_embedding = raw_model_data_->weight(0);
  llama_layers_->embedding_layer_->set_weight(0, {std::abs(config_->vocab_size_), \
       config_->dim_},  weight_embedding, cpu_device_type);
3. 我们再来回顾一下set_weight的定义，虽然在前面的课程中已经讲过了；
base::Status LayerParam::set_weight(int32_t idx, const std::vector<int32_t>& dims,
                                    const void* weight_ptr, base::DeviceType device_type) {
    CHECK_GE(idx, 0);
    CHECK_LT(idx, weights_.size());
    CHECK_NE(weight_ptr, nullptr);
        
    // 3.1 计算权重的数量，计算方法是将维度依次累计相乘
    size_t size = std::accumulate(dims.begin(), dims.end(), sizeof(float), std::multiplies<>());
    // 3.2 将权重指针（这里来自于weight_data）赋值给一个buffer，buffer就是我们以往课程中说过的，用于管理内存资源的一个类。
    // 3.2 buffer是可以共享计数的，当没有使用者且拥有这块指针的所有权时buffer会将指针释放，以获得自动管理内存的功能。
    std::shared_ptr<base::Buffer> buffer =
        std::make_shared<base::Buffer>(size, nullptr, (void*)(weight_ptr), true);
    if (device_type != base::DeviceType::kDeviceUnknown) {
        buffer->set_device_type(device_type);
    }
    // 3.3 创建每个算子关联的权重，它是一个张量。默认赋值的张量在cpu上的，
    // 3.3 等下我们需要把它upload到gpu中。
    tensor::Tensor weight(base::DataType::kDataTypeFp32, dims);
    weight.set_device_type(device_type);
    CHECK(weight.assign(buffer));
    weights_.at(idx) = weight;
    return base::error::Success();
}
        3.1 首先计算权重的数量，通过维度累乘的方式去得到；
        3.2 随后再将这块权重指针赋值给一个buffer实例，做到对内存的自动管理，这里的内存weight_data是由mmap自动映射的，所以buffer不需要管理它的生命周期；
        3.3 将管理权重数据的buffer实例赋值给该层作为它的权重。此处的权重还是cpu类型的，等下我们需要把它upload到显存中。创建Linear算子同理，同样是将weight_data移动到一定的偏移位置中：
// create weight matrix for key
for (int32_t i = 0; i < config_->layer_num_; ++i) {
    auto wk = std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim);
    wk->set_weight(0, {config_->kv_dim_, dim}, this->raw_model_data_->weight(pos), cpu_device_type);
    llama_layers_->wk_layers_.push_back(wk);
    pos += config_->kv_dim_ * dim;
}
同样是新创建一个算子wk，随后再将当前weight_data的偏移作为权重赋值给该算子，也就是调用wk->set_weight，我们看到第6行会对权重的位置做一个偏移，以方便下一个算子找到属于自己的权重区域，流程也符合以下的：
1. 创建一个算子层，例如op::MatmulLayer；
2. 将权重赋值给改算子，使用set_weight方法
每个带参数的算子中有如下的结构，其中weight是算子的类内变量，作为它的权重使用。
class Layer{
    ...
    ...
    // 权重
    tensor::Tensor weight;
}
        在set_weight方法中我们先将weight_data指针赋值给一个新初始化的Buffer用于管理内存指针的生命周期，但是由于这里是用mmap打开的，不需要Buffer进行管理，所以我们将Buffer中use_external置为true。
std::shared_ptr<base::Buffer> buffer =
    std::make_shared<base::Buffer>(size, nullptr, (void*)(weight_ptr), true);
if (device_type != base::DeviceType::kDeviceUnknown) {
    buffer->set_device_type(device_type);
}
3. 随后我们再将该Buffer赋值给一个算子实例(Layer类)的weight类内变量，至此就完成了一个带参数算子的初始化。
4. 随后使pos移动到下一个算子对应权重的起始位置。
4. 创建不带权重的层
在调用create_param_layers()方法创建完带参数的参数之后，我们就要开始创建不带权重参数的算子层，这里更加简单，因为不再需要读取权重。
void LLama2Model::create_nonparam_layers() {
    CHECK(llama_layers_ != nullptr);
    // 创建一个rope层
    llama_layers_->rope_layer_ = std::make_shared<op::RoPELayer>(
        device_type_, config_->dim_, config_->kv_dim_, config_->head_size_);

    // 创建一个多头注意力层
    llama_layers_->mha_layer_ = std::make_shared<op::MultiHeadAttention>(
        device_type_, 0, config_->kv_mul_, 
        config_->kv_dim_, config_->seq_len_, config_->head_num_, config_->head_size_);

    // 创建add层
    llama_layers_->add_layer_ = std::make_shared<op::VecAddLayer>(device_type_);

    llama_layers_->swiglu_layer_ =
        std::make_shared<op::SwiGLULayer>(device_type_, config_->hidden_dim_);
}
对于不带参数的算子层，直接对它们进行初始化就可以了，而没有移动weight_data并使用set_weight赋值权重的过程。
5. 单元测试
我们准备了一个模型test.bin，在这个模型中的后面部分是一个线性算子的权重，我们要读取它作为一个线性层算子的权重。我们在上文中说过在权重部分这个模型会存放一个128×16维度的数据，大小是从0...1024。
---------
dim,hidden_dim,layer_num，...  前面的28个字节
group size
---------
float 权重
这里是0,1,2,3,4,...,1024
---------
所以我们像上文说的那样把先把权重读取进来，也就是下文中的weight_data。
TEST(test_load, create_matmul) {
  std::string model_path = "./tmp/test.bin";
  int32_t fd = open(model_path.data(), O_RDONLY);
  ASSERT_NE(fd, -1);

  FILE* file = fopen(model_path.data(), "rb");
  ASSERT_NE(file, nullptr);

  auto config = model::ModelConfig{};
  fread(&config, sizeof(model::ModelConfig), 1, file);

  fseek(file, 0, SEEK_END);
  auto file_size = ftell(file);
        
  // 映射权重
  void* data = mmap(nullptr, file_size, PROT_READ, MAP_PRIVATE, fd, 0);
  float* weight_data =
      reinterpret_cast<float*>(static_cast<int8_t*>(data) + sizeof(model::ModelConfig));
        
  for (int i = 0; i < config.dim * config.hidden_dim; ++i) {
    ASSERT_EQ(*(weight_data + i), float(i));
  }
  /**                                  1
   *    1 2 3 4 5 6 ... 1024           1
   *                                   1
   *                                   1
   */
随后在该单元测试内，创建一个全连接算子wq，并将权重装入到一个16×128的张量之中：
auto wq = std::make_shared<op::MatmulLayer>(base::DeviceType::kDeviceCPU, config.dim,
                                            config.hidden_dim, false);
我们依次解释一下这里的参数：
1. 算子的设备类型，表明用那种设备进行运算；
2. 第2和第3个参数分别表示算子的out_features和in_features；
3. 最后的参数表明该算子不为量化算子，如果是量化算子的话在赋值权重的时候就需要额外读取group size个伸缩系数(scales)。
为wq算子载入权重
wq->set_weight(0, {config.dim, config.hidden_dim}, weight_data, base::DeviceType::kDeviceCPU);
为wq算子设置输入和输出张量
wq->set_input(0, tensor);      //设置输入
wq->set_output(0, out_tensor); // 设置输出
wq->set_weight(0, {config.dim, config.hidden_dim}, weight_data,
         base::DeviceType::kDeviceCPU); // 设置权重
算子推理和结果验证，实现过程我们还没讲，我们先看看是怎么用的。
wq->forward();

/** python code:
   *  w = np.arange(0,128 * 16).reshape(16, 128)
   *  input = np.ones(128)
   *  out = w@input
   */
ASSERT_EQ(out[0], 8128);
ASSERT_EQ(out[1], 24512);
ASSERT_EQ(out[14], 237504);
ASSERT_EQ(out[15], 253888);
计算过程就是将权重和输入相乘，如果用python模拟计算过程则有：
import numpy as np
w = np.arange(0,128 * 16).reshape(16, 128)
input = np.ones(128)
out = w@input
