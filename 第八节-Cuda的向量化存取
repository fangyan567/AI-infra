第八节-Cuda的向量化存取

1. 模型的量化回顾
我们先回顾一下之前写RMSNorm时的流程，先是需要求出所有输入数据的平方和，也就是
$$sum = \sum_{i=1}^n{{input_i }^2}$$

原先的Cuda核函数规约流程是这样的：在一个BLOCK中，每个线程负责计算对应数据的局部和。例如，线程0负责对输入数据input中下标为0、16、32、48、64等的数据进行求和；线程1则负责下标为1、17、33、49等的数据求和。换句话说，每个线程都计算出了一段数据的局部和。接下来，我们需要将这些线程的局部和累加起来，得到一个全局和，也就是公式中的sum。
一个线程块(Block)包含64个线程，而数据总量为1024个，因此每个线程需要处理16个数据。每个线程都会计算出几个数据的局部和。例如，线程0负责的是下标为0、16、32、48、64、80等位置的数据的局部和。线程0存储的是下标为0、16、32、48等数据的局部和； 线程1存储的是下标为1、17、33、49等数据的局部和； 线程16存储的是下标为16、32、48、64等数据的局部和。 我们需要将线程0到线程63的局部和累加起来。在第一个循环中，我们将线程0与线程32的值相加； 在第二个循环中，我们将线程0与线程16的值相加； 在第三个循环中，我们将线程0与线程8的值相加； 如此继续，直到最后一个循环，此时线程0中的值将等于所有局部和的累加值。
template<const int NUM_THREADS=128>
__device__ __forceinline__ float block_reduce_sum(float val) {
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  int warp = threadIdx.x / WARP_SIZE;
  int lane = threadIdx.x % WARP_SIZE;
  static __shared__ float shared[NUM_WARPS];
  // 1024个数据，有128个线程
  // 一个线程处理8个数据
    
  // 32个线程作为一个集合进行规约，0+16,1+17，线程2+18
  // 这里的val经过多轮循环之后，线程0的累计值
  // val是32个线程各自局部和之累加
  val = warp_reduce_sum<WARP_SIZE>(val);
  if (lane == 0) shared[warp] = val;
  __syncthreads();
    // 128个线程，32个线程一个规约，还有规约后有4个值
  val = (lane < NUM_WARPS) ? shared[lane] : 0.0f;
    // 再把4个组的值最后规约一遍得到最终的结果
    
    // 线程0 存放规约值0，线程0-31的局部和
    // 线程1 存放规约值1，线程32-63的局部和
    // 线程2 存放规约值2，线程64-95的局部和
    // 线程3 存放规约值3，线程96-127的局部和
    // 线程4-到线程31 都存放了0值
  val = warp_reduce_sum<NUM_WARPS>(val);
  return val;
}

template<const int kWarpSize = WARP_SIZE>
__device__ __forceinline__ float warp_reduce_sum(float val) {
  #pragma unroll
  for (int mask = kWarpSize >> 1; mask >= 1; mask >>= 1) {
    val += __shfl_xor_sync(0xffffffff, val, mask);
  }
  return val;
}

       其中，传入的参数val代表每个线程的局部和，warp表示warp组的编号，而lane则表示线程在warp组内的偏移量。我们首先将32个线程视为一个warp，并对这32个线程进行组内规约求和。
例如，假设我们有160个线程，每32个线程组成一个warp，那么总共会有五个warp。
在第一个warp的规约过程中，第一轮循环会将线程0与线程16的局部和相加，将线程1与线程17的局部和相加，以此类推，直到线程8与线程24的局部和相加。随后，在第二轮循环中，规约的跨度将减半，因此会将线程0与线程8的局部和进行求和。
我们知道，在第一轮循环中，线程0已经累计了它自己的结果和线程16的结果，而线程8则累计了它自己的结果和线程24的结果。因此，在第二轮循环中，当线程0与线程8相加时，线程0中将累计下标为0、16、8、24的值。为了帮助大家更好地理解这个过程，我们可以参考以下的图示：图中展示了一个warp的情况，以8个线程（我们简化了线程的个数）为一个单位进行规约，其余warp的规约情况同理。
以8个线程为一组进行规约的过程如下描述：
在第一轮循环中：
- 线程0的计算结果是线程0和线程4的和，其中线程0原本记录的是位置0的值，线程4记录的是位置4的值，因此线程0现在记录的是位置0和位置4的值的总和。
- 线程1和线程5的计算结果是位置1和位置5的值的总和。
- 线程2和线程6的计算结果是位置2和位置6的值的总和。
- 线程3和线程7的计算结果是位置3和位置7的值的总和。
在第二轮循环中：
- 线程0的计算结果是线程0和线程2的和，即位置0、位置4、位置2和位置6的值的总和。
- 线程1的计算结果是线程1和线程3的和，即位置1、位置5、位置3和位置7的值的总和。
在第三轮循环中：
- 线程0的计算结果是线程0和线程1的和，这样线程0就记录了位置0、位置4、位置2、位置6、位置1、位置3、位置5和位置7的所有值的总和。
通过这三轮循环，线程0最终累积了这一组8个线程对应的所有位置的局部和的总和。
[图片]
在上图中，每一层都是规约的不同状态，可以看出第1层规约的时候的步长是4，到第2层开始的规约时的步长等于2，等到第3层的规约时步长等于1，每层的每次规约得在累计不同位置上的局部和。执行逐层规约的就是warp_reduce_sum函数：
template<const int kWarpSize = WARP_SIZE>
__device__ __forceinline__ float warp_reduce_sum(float val) {
  #pragma unroll
  for (int mask = kWarpSize >> 1; mask >= 1; mask >>= 1) {
    val += __shfl_xor_sync(0xffffffff, val, mask);
  }
  return val;
}
在每次for循环中，线程都会与一定跨度的另一个线程交换数据并累加。进入下一个循环时，每次的跨度会变为前一次的一半，直到最终的跨度变为1。在这个跨度下，将线程0和线程1中的数据相加，得到最终的全局和结果。
如前所述，我们以32个线程为一个单位（warp）来进行规约操作。完成所有warp的规约后，我们还需要将这5个warp的结果再次进行求和，以得到最终的全局和。我们的解决方案是将warp0的结果存储在线程0中，将warp1的结果存储在线程1中，以此类推，直到将warp4的结果存储在线程4中，请见第13到17行。对于剩余的线程编号4到31，我们将它们的数据置为空。最后，我们再进行一次warp内的规约操作，这样就可以得到最终的全局和结果。
2. 向量化存取
假设我们现在有35个数据，我们原先的方式是逐个取出求平方和，见以下代码：
float sum = 0.0f;
for (int i = tid; i < size; i += blockDim.x) {
   sum += in[i] * in[i];
}
在CUDA中，向量化存取是一种技术，它通过使用向量数据类型（例如CUDA内置的float2、float3、float4等）来同时加载或存储多个数据元素。这种方法能够显著提高内存吞吐量，并减少内存访问的次数，进而提升程序的整体性能。在上面的代码中，每次加载的元素仅为一个，这并没有充分利用带宽，因此还有提升空间。通过向量化存取，我们可以同时处理多个元素，从而更有效地利用内存带宽，进一步提高代码的效率。
假设的35个数据，我们可以以这样的形式进行读取
线程0：0，32，64
线程0：[0, 1, 2, 3] ，[32, 33, 34, 35] 循环的次数会减少
1. 0 - 3
2. 4 - 7
3. 8 - 11
4. ...
5. 28 - 31
直到最后一组，因为不能构成4个单位，所以再进行1个1个的处理。
static __global__ void row_rmsnorm_f32(float* in, float* wei, float* out, int size, float eps) {
  const int tid = threadIdx.x;

  constexpr int pack_size = 4;
  const int pack_num = size / pack_size;
  const int pack_off = pack_size * pack_num;

  float sum = 0.0f;
  float4* in_pack = reinterpret_cast<float4*>(in);

  for (int i = tid; i < pack_num; i += blockDim.x) {
    float4 in_float4 = *(in_pack + i);
    sum += in_float4.x * in_float4.x;
    sum += in_float4.y * in_float4.y;
    sum += in_float4.z * in_float4.z;
    sum += in_float4.w * in_float4.w;
  }

  for (int i = pack_off + tid; i < size; i += blockDim.x) {
    sum += in[i] * in[i];
  }
从上述例子中，我们可以了解到，当数据大小为35时，计算得出的pack_num值为32。以线程0为例，即当tid为0时，我们会一次性读取位置0至3的数据，并将其存储在名为in_float4的变量中。这个变量类型为float4，包含四个成员：x、y、z和w，它们分别代表从位置0到位置3的数据。接下来，我们会同时对这四个数据执行求平方和的操作。
在这里，以4个数据为一组 数据总数为35个，分组如下： 0-3, 4-7, 8-11, 28-31 剩余32-34。同样地，在写入数据时，我们也以每4个数据为一个单位进行操作。以下是相应的代码实现：
float4* wei_pack = reinterpret_cast<float4*>(wei);
float4* out_pack = reinterpret_cast<float4*>(out);
for (int i = tid; i < pack_num; i += blockDim.x) {
    float4 in_float4 = *(in_pack + i);
    float4 wei_float4 = *(wei_pack + i);
    *(out_pack + i) =
        make_float4(scale * in_float4.x * wei_float4.x, scale * in_float4.y * wei_float4.y,
                    scale * in_float4.z * wei_float4.z, scale * in_float4.w * wei_float4.w);
}
和处理输入的时候同理，我们也是以多字节的方式去读取输入，并将输入与scale和weight进行相乘之后再以多字节的方式写入到输出中。
3. 性能测试
3.1 没有向量化存取
[图片]
[图片]
3.2 向量化存取
[图片]
可以看出内存吞吐率和L2的吞吐率显著增加了，这是因为：
1. 每次内存访问都需要一定的时间和能量开销，这称为事务开销。当使用向量化存取时，多个数据元素在一个内存事务中被加载或存储，这意味着这些元素共享了事务开销，从而减少了总的内存事务次数。
2. 另外也是由于，GPU的内存带宽是有限的。通过向量化存取，可以使得每次内存传输携带更多的数据，从而在相同的时间内传输更多的信息，充分利用内存带宽。
3. 向量化指令可以一次性操作多个数据元素，减少了所需的指令数量。这不仅可以减少指令解码和执行的负担，还可以提高指令的吞吐量。
4. GPU的内存子系统针对连续的内存访问模式进行了优化。向量化存取通常涉及连续的内存地址，这样可以最大化缓存的利用率，减少缓存未命中。

[图片]
可以从上面看出执行的指令数量确实得到了减少。
